# MediOps Python Application Troubleshooting Guide

This document contains solutions for common Python issues in MediOps platform.

## ConnectionRefusedError
**Problem:** Unable to connect to PostgreSQL database due to connection being refused, typically caused by incorrect host/port configuration or database service not running.
**Solution:**
```python
try:
    conn = psycopg2.connect(
        dbname="patient_records",
        user="mediadmin",
        password="secure_password",
        host="db.mediops.internal",
        port="5432",
        connect_timeout=10
    )
except psycopg2.OperationalError as e:
    logging.error(f"Database connection failed: {e}")
    # Check if database is running
    import subprocess
    subprocess.run(["systemctl", "status", "postgresql"])
```
**Module:** patient_data_service

## ConnectionPoolExhausted
**Problem:** Database connection pool has reached maximum capacity, causing new connection requests to fail during peak load.
**Solution:**
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    'postgresql+psycopg2://mediadmin:secure_password@db.mediops.internal/patient_records',
    poolclass=QueuePool,
    pool_size=20,  # Increase from default 5
    max_overflow=30,  # Allow 30 connections beyond pool_size
    pool_timeout=30,  # Wait up to 30 seconds for available connection
    pool_recycle=1800  # Recycle connections after 30 minutes
)
```
**Module:** appointment_scheduler

## CredentialExpiredError
**Problem:** Database connection fails because the service account credentials used for authentication have expired.
**Solution:**
```python
from vault_client import VaultClient

def get_refreshed_db_credentials():
    vault = VaultClient(token=os.environ.get('VAULT_TOKEN'))
    credentials = vault.get_credentials('database/patient_records')
    
    return {
        'user': credentials['username'],
        'password': credentials['password'],
        'host': 'db.mediops.internal',
        'database': 'patient_records'
    }

# Use refreshed credentials
import pymysql
conn = pymysql.connect(**get_refreshed_db_credentials())
```
**Module:** billing_service

## DeadlockDetected
**Problem:** Transaction failed due to deadlock detection in the database when multiple services attempt to update the same patient records simultaneously.
**Solution:**
```python
from sqlalchemy.exc import OperationalError
import time

def update_patient_record(session, patient_id, data, max_retries=3):
    retries = 0
    while retries < max_retries:
        try:
            session.begin()
            patient = session.query(Patient).with_for_update().filter_by(id=patient_id).first()
            for key, value in data.items():
                setattr(patient, key, value)
            session.commit()
            return True
        except OperationalError as e:
            if "deadlock detected" in str(e).lower():
                session.rollback()
                retries += 1
                time.sleep(0.5 * retries)  # Exponential backoff
            else:
                raise
    raise Exception(f"Failed to update patient {patient_id} after {max_retries} attempts due to deadlocks")
```
**Module:** patient_update_service

## ConnectionTimeoutError
**Problem:** Database queries are timing out due to network latency between application servers and database cluster during high traffic periods.
**Solution:**
```python
import pymongo
from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError

client = pymongo.MongoClient(
    "mongodb://mongo.mediops.internal:27017/",
    serverSelectionTimeoutMS=10000,  # Increase timeout to 10 seconds
    socketTimeoutMS=45000,  # Socket timeout for operations
    connectTimeoutMS=20000,  # Connection timeout
    retryWrites=True,  # Automatically retry write operations
    w='majority'  # Wait for write acknowledgment from majority of replicas
)

try:
    # Ping the server to verify connection before performing operations
    client.admin.command('ping')
    db = client.medical_records
except (ConnectionFailure, ServerSelectionTimeoutError) as e:
    logging.critical(f"Database connection failed: {e}")
    # Implement circuit breaker pattern
    notify_ops_team("Database connection failure", str(e))
```
**Module:** medical_records_service

## Error
**Problem:** Missing required configuration for patient data encryption in settings.py
**Solution:** 
```python
# Add encryption configuration
ENCRYPTION = {
    'ALGORITHM': 'AES',
    'KEY_PATH': '/etc/mediops/keys/patient_data.key',
    'ENABLE_AT_REST': True,
    'ENABLE_IN_TRANSIT': True
}
```
**Module:** security_manager

## Error
**Problem:** Database connection timeout value too low for large patient record batches
**Solution:** 
```python
# Increase connection timeout and add retry logic
DB_CONFIG = {
    'host': 'patient-db.mediops.internal',
    'port': 5432,
    'connection_timeout': 120,  # Increased from 30
    'max_retries': 3,
    'retry_delay': 5
}
```
**Module:** database_connector

## Error
**Problem:** Log rotation configuration missing, causing disk space issues with patient data logs
**Solution:** 
```python
# Configure proper log rotation
LOGGING = {
    'version': 1,
    'handlers': {
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/var/log/mediops/patient_data.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 20,
            'formatter': 'standard'
        }
    }
}
```
**Module:** logging_service

## Error
**Problem:** Missing HIPAA compliance settings for data retention policy
**Solution:** 
```python
# Add HIPAA compliance data retention settings
COMPLIANCE = {
    'data_retention_days': 2555,  # 7 years
    'auto_archive': True,
    'archive_path': '/data/mediops/archives/',
    'purge_after_archive': True,
    'audit_enabled': True
}
```
**Module:** compliance_manager

## Error
**Problem:** Incorrect concurrency settings causing race conditions in patient record updates
**Solution:** 
```python
# Fix concurrency settings with proper locking
PROCESSING = {
    'max_workers': 4,
    'use_distributed_lock': True,
    'lock_timeout': 60,
    'lock_retry_count': 3,
    'transaction_isolation': 'SERIALIZABLE'
}
```
**Module:** record_processor

## Memory Leak in Connection Pool
**Problem:** Connection objects to the EHR system aren't being properly closed, causing memory leaks during high-volume operations.
**Solution:** 
```python
# Before: Connections not properly managed
def get_patient_data(patient_id):
    conn = ehr_connection_pool.get_connection()
    data = conn.query(f"SELECT * FROM patients WHERE id = {patient_id}")
    return data  # Connection never released

# After: Using context manager to ensure connections are returned
def get_patient_data(patient_id):
    with ehr_connection_pool.connection() as conn:
        data = conn.query(f"SELECT * FROM patients WHERE id = {patient_id}")
        return data  # Connection automatically released
```
**Module:** connection_manager.py

## Excessive Memory Usage in Data Processing
**Problem:** Loading entire patient records into memory when only specific fields are needed, causing OOM errors with large datasets.
**Solution:** 
```python
# Before: Loading entire records
def process_patient_batch(batch_ids):
    patients = [ehr_client.get_full_patient_record(pid) for pid in batch_ids]
    for patient in patients:
        process_demographics(patient)  # Only needs demographic data

# After: Selective field retrieval
def process_patient_batch(batch_ids):
    patients = [ehr_client.get_patient_demographics(pid) for pid in batch_ids]
    for patient in patients:
        process_demographics(patient)
```
**Module:** data_processor.py

## CPU Bottleneck in Parallel Processing
**Problem:** Inefficient use of multiprocessing causing thread contention and high CPU usage when processing multiple patient records.
**Solution:** 
```python
# Before: Naive parallel implementation
def process_records(record_ids):
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:  # Too many workers
        results = list(executor.map(process_single_record, record_ids))
    return results

# After: Optimized worker count with chunking
def process_records(record_ids):
    optimal_workers = min(32, os.cpu_count() + 4)  # Avoid excessive context switching
    chunk_size = max(1, len(record_ids) // optimal_workers)
    
    with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
        results = list(executor.map(process_single_record, record_ids, chunksize=chunk_size))
    return results
```
**Module:** batch_processor.py

## Inefficient JSON Serialization
**Problem:** Repeatedly serializing and deserializing large JSON patient objects during API interactions, causing significant performance degradation.
**Solution:** 
```python
# Before: Repeated serialization
def update_patient_records(patients):
    for patient in patients:
        json_data = json.dumps(patient)
        response = requests.post(EHR_API_URL, data=json_data)
        updated = json.loads(response.content)
        patient.update(updated)

# After: Using connection pooling and keeping objects serialized
@lru_cache(maxsize=100)
def get_serializer(object_type):
    return JsonSerializer(object_type)

def update_patient_records(patients):
    serializer = get_serializer('patient')
    session = requests_session_pool.get_session()
    
    for patient in patients:
        json_data = serializer.serialize(patient)
        response = session.post(EHR_API_URL, data=json_data)
        patient.update(serializer.deserialize(response.content))
```
**Module:** api_client.py

## Large Object Cache Overflow
**Problem:** Caching too many large patient record objects without size limits, leading to memory pressure and eventual OOM errors.
**Solution:** 
```python
# Before: Unbounded cache
patient_cache = {}
def get_patient(patient_id):
    if patient_id not in patient_cache:
        patient_cache[patient_id] = fetch_patient_from_ehr(patient_id)
    return patient_cache[patient_id]

# After: Size-aware LRU cache with TTL
from cachetools import TTLCache, cached

# 1000 patients max, 30 minute TTL
patient_cache = TTLCache(maxsize=1000, ttl=1800)

@cached(cache=patient_cache)
def get_patient(patient_id):
    return fetch_patient_from_ehr(patient_id)
```
**Module:** cache_manager.py

## AuthenticationError
**Problem:** Invalid JWT token provided for API authentication.
**Solution:** 
```python
import jwt
from datetime import datetime, timedelta

def create_valid_token(user_id, secret_key):
    payload = {
        'user_id': user_id,
        'exp': datetime.utcnow() + timedelta(hours=1),
        'iat': datetime.utcnow()
    }
    return jwt.encode(payload, secret_key, algorithm='HS256')

# Usage
token = create_valid_token(user_id='12345', secret_key=app_config.SECRET_KEY)
headers = {'Authorization': f'Bearer {token}'}
```
**Module:** auth.token_validator

## PermissionDeniedError
**Problem:** User lacks required role to access claims adjudication endpoints.
**Solution:** 
```python
def check_claims_permission(user):
    required_roles = ['claims_adjudicator', 'claims_admin']
    if not any(role in user.roles for role in required_roles):
        raise PermissionDeniedError(
            f"User {user.id} requires one of these roles: {', '.join(required_roles)}"
        )
    return True

# Usage in view
@app.route('/api/claims/<claim_id>/approve', methods=['POST'])
@login_required
def approve_claim(claim_id):
    check_claims_permission(current_user)
    # Process claim approval
```
**Module:** auth.permissions

## EncryptionKeyError
**Problem:** Unable to decrypt sensitive patient data due to missing or invalid encryption key.
**Solution:** 
```python
from cryptography.fernet import Fernet, InvalidToken

def decrypt_patient_data(encrypted_data, key_manager):
    try:
        key = key_manager.get_active_key()
        if not key:
            raise EncryptionKeyError("No active encryption key found")
            
        f = Fernet(key)
        return f.decrypt(encrypted_data.encode()).decode()
    except InvalidToken:
        raise EncryptionKeyError("Data cannot be decrypted with available keys")

# Usage
try:
    patient_ssn = decrypt_patient_data(claim.encrypted_ssn, key_manager)
except EncryptionKeyError as e:
    log.error(f"Failed to decrypt patient data: {str(e)}")
    # Handle gracefully
```
**Module:** security.encryption

## SessionExpiredError
**Problem:** User session has timed out while processing a multi-step claims review.
**Solution:** 
```python
def verify_session_active(session, max_idle_minutes=30):
    if 'last_activity' not in session:
        raise SessionExpiredError("No activity timestamp in session")
        
    last_activity = datetime.fromisoformat(session['last_activity'])
    idle_time = datetime.utcnow() - last_activity
    
    if idle_time > timedelta(minutes=max_idle_minutes):
        raise SessionExpiredError("Session expired due to inactivity")
    
    # Update last activity time
    session['last_activity'] = datetime.utcnow().isoformat()
    return True

# Usage in middleware
@app.before_request
def check_session_expiry():
    if current_user.is_authenticated:
        try:
            verify_session_active(session)
        except SessionExpiredError:
            logout_user()
            return redirect(url_for('login', next=request.url))
```
**Module:** auth.session_manager

## CSRFValidationError
**Problem:** Missing or invalid CSRF token in claims form submission.
**Solution:** 
```python
from flask_wtf.csrf import CSRFProtect, CSRFError

csrf = CSRFProtect(app)

@app.errorhandler(CSRFError)
def handle_csrf_error(e):
    return jsonify({
        'error': 'CSRF validation failed',
        'message': 'Please refresh the page and try again',
        'code': 'csrf_error'
    }), 400

# In forms
class ClaimSubmissionForm(FlaskForm):
    # Form fields
    patient_id = StringField('Patient ID', validators=[DataRequired()])
    procedure_code = StringField('Procedure Code', validators=[DataRequired()])
    
    # CSRF protection is automatically included by FlaskForm
    
# In template
<form method="post" action="{{ url_for('submit_claim') }}">
    {{ form.csrf_token }}
    <!-- Form fields -->
</form>
```
**Module:** security.csrf_protection